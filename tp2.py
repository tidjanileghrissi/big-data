import pandas as pd
import dask.dataframe as dd
import time
import os
import psutil
import pyarrow as pa
import pyarrow.parquet as pq
import zipfile
from dask.distributed import Client
import sys

sys.stdout.reconfigure(encoding='utf-8')

# ğŸ“Œ Ù‚ÙŠØ§Ø³ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
def measure_memory(repeats=10, delay=0.5):
    readings = []
    process = psutil.Process(os.getpid())
    for _ in range(repeats):
        mem = process.memory_info().rss / (1024 ** 2)
        readings.append(mem)
        time.sleep(delay)
    return max(readings)

# ğŸ’¾ ÙÙƒ Ø¶ØºØ· Ù…Ù„Ù CSV Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¯Ø§Ø®Ù„ ZIP
def extract_csv_from_zip(zip_path, extract_to):
    if os.path.exists(zip_path):
        print(f"ğŸ“‚ ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø¹Ù† {zip_path}...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
        print("âœ… ØªÙ… ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø¨Ù†Ø¬Ø§Ø­!")

# ğŸ’® ØªØ­ÙˆÙŠÙ„ CSV Ø¥Ù„Ù‰ Parquet
def convert_to_parquet(input_file: str, output_file: str, chunk_size: int = 5000) -> bool:
    if not os.path.exists(output_file):
        print(f"ğŸ”„ Ø¬Ø§Ø±ÙŠ ØªØ­ÙˆÙŠÙ„ {os.path.basename(input_file)} Ø¥Ù„Ù‰ Parquet...")
        writer = None
        try:
            for i, chunk in enumerate(pd.read_csv(input_file, chunksize=chunk_size, low_memory=False)):
                chunk = chunk.apply(lambda x: x.astype(str) if x.dtype == object else x)
                table = pa.Table.from_pandas(chunk)
                if not writer:
                    writer = pq.ParquetWriter(output_file, schema=table.schema, compression='snappy', use_dictionary=True)
                writer.write_table(table)
                print(f"âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¬Ø²Ø¡ {i+1}")
            print(f"ğŸ‰ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parquet Ø§ÙƒØªÙ…Ù„! Ø§Ù„Ù…Ù„Ù: {output_file}")
            return True
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­ÙˆÙŠÙ„: {str(e)}")
            if writer:
                writer.close()
                os.remove(output_file)
            return False
        finally:
            if writer:
                writer.close()
    else:
        print(f"âš ï¸ Ø§Ù„Ù…Ù„Ù {output_file} Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ù‹Ø§")
        return True

# ğŸ“Š Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Pandas (ØªØ¬Ø²Ø¦Ø©)
def pandas_chunking(file_name, chunk_size=10000):
    start_mem = measure_memory()
    start_time = time.time()
    total_sum = {}
    total_count = {}
    try:
        for chunk in pd.read_csv(file_name, chunksize=chunk_size, low_memory=False):
            grouped = chunk.groupby("event_type")["price"]
            for name, group in grouped:
                total_sum[name] = total_sum.get(name, 0) + group.sum()
                total_count[name] = total_count.get(name, 0) + group.count()
        final_result = {name: total_sum[name] / total_count[name] for name in total_sum}
        return {
            'Method': 'Pandas Chunking',
            'Time (Seconds)': round(time.time() - start_time, 2),
            'Memory (MB)': round(measure_memory() - start_mem, 2)
        }
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ÙÙŠ Pandas Chunking: {e}")
        return {'Method': 'Pandas Chunking', 'Time (Seconds)': None, 'Memory (MB)': None}

# ğŸš€ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Dask
def dask_processing(file_name):
    client = None
    try:
        client = Client(n_workers=1, threads_per_worker=1, memory_target_fraction=0.9, memory_spill_fraction=0.9)
        start_mem = measure_memory(repeats=10, delay=0.5)
        start_time = time.time()
        df = dd.read_csv(file_name, low_memory=False, blocksize='8MB').persist()
        for _ in range(3):
            df['price'] = df['price'].astype(float)
            df['discounted_price'] = df['price'] * 0.8
        grouped = df.groupby("event_type").agg(avg_price=('price', 'mean'), max_price=('price', 'max'), min_price=('price', 'min')).persist()
        time.sleep(10)
        end_mem = measure_memory(repeats=20, delay=1)
        return {
            'Method': 'Dask Processing', 'Time (Seconds)': round(time.time() - start_time, 2), 'Memory (MB)': round(end_mem - start_mem, 2)
        }
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ ÙÙŠ Dask Processing: {e}")
        return {'Method': 'Dask Processing', 'Time (Seconds)': None, 'Memory (MB)': None}
    finally:
        if client:
            client.close()

# ğŸ“‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Parquet
def parquet_compression(file_name):
    start_mem = measure_memory()
    start_time = time.time()
    try:
        parquet_file = pq.ParquetFile(file_name)
        total_sum = {}
        total_count = {}
        for batch in parquet_file.iter_batches(columns=['event_type', 'price'], batch_size=100000):
            df_batch = batch.to_pandas()
            grouped = df_batch.groupby("event_type")["price"]
            for name, group in grouped:
                total_sum[name] = total_sum.get(name, 0) + group.sum()
                total_count[name] = total_count.get(name, 0) + group.count()
        avg_price = {k: total_sum[k]/total_count[k] for k in total_sum}
        return {
            'Method': 'Parquet Compression',
            'Time (Seconds)': round(time.time() - start_time, 2),
            'Memory (MB)': round(measure_memory() - start_mem, 2)
        }
    except Exception as e:
        print(f"âŒ ÙØ´Ù„ Ù‚Ø±Ø§Ø¡Ø© Parquet: {e}")
        return {'Method': 'Parquet Compression', 'Time (Seconds)': None, 'Memory (MB)': None}

# ğŸ—‘ï¸ Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
if __name__ == "__main__":
    base_path = r"C:\Users\LAPTA\OneDrive\Desktop\LEGHRISSI TP\2019-Oct.csv (1)"

    # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¶ØºÙˆØ· ÙˆÙÙƒ Ø¶ØºØ·Ù‡
    zip_file = os.path.join(base_path, "2019-Oct.csv.zip")
    extract_csv_from_zip(zip_file, base_path)

    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¨Ø¹Ø¯ ÙÙƒ Ø§Ù„Ø¶ØºØ·
    input_csv = os.path.join(base_path, "2019-Oct.csv")
    output_parquet = os.path.join(base_path, "2019-Oct.parquet")

    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù…ÙˆØ¬ÙˆØ¯
    if not os.path.exists(input_csv):
        print(f"âŒ Ø§Ù„Ù…Ù„Ù {input_csv} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…Ø³Ø§Ø±!")
    else:
        success = convert_to_parquet(input_csv, output_parquet)
        if success:
            results = [
                pandas_chunking(input_csv),
                dask_processing(input_csv),
                parquet_compression(output_parquet)
            ]
            
            # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            print("\nğŸ“Š **Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„:**")
            for result in results:
                print(result)
        else:
            print("âŒ ÙØ´Ù„ ÙÙŠ ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª!")
